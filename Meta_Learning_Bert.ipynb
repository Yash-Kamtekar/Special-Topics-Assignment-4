{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yash-Kamtekar/Special-Topics-Assignment-4/blob/main/Meta_Learning_Bert.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ty1f5BQfCOq",
        "outputId": "a6b6f868-8a3a-4237-fe73-e226e323c1fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdYRw0JhfDHc",
        "outputId": "8c18a222-f5b0-4d8e-890b-55e93300f4ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'text': \"GOOD LOOKING KICKS IF YOUR KICKIN IT OLD SCHOOL LIKE ME. AND COMFORTABLE. AND RELATIVELY CHEAP. I'LL ALWAYS KEEP A PAIR OF STAN SMITH'S AROUND FOR WEEKENDS\",\n",
              "  'label': 'positive',\n",
              "  'domain': 'apparel'},\n",
              " {'text': 'These sunglasses are all right. They were a little crooked, but still cool..',\n",
              "  'label': 'positive',\n",
              "  'domain': 'apparel'},\n",
              " {'text': \"I don't see the difference between these bodysuits and the more expensive ones. Fits my boy just right\",\n",
              "  'label': 'positive',\n",
              "  'domain': 'apparel'},\n",
              " {'text': 'Very nice basic clothing. I think the size is fine. I really like being able to find these shades of green, though I have decided the lighter shade is really a feminine color. This is the only brand that I can find these muted greens',\n",
              "  'label': 'positive',\n",
              "  'domain': 'apparel'},\n",
              " {'text': 'I love these socks. They fit great (my 15 month old daughter has thick ankles) and she can zoom around on the kitchen floor and not take a nose dive into things. :',\n",
              "  'label': 'positive',\n",
              "  'domain': 'apparel'}]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "import json\n",
        "from random import shuffle\n",
        "reviews = json.load(open('/content/drive/MyDrive/297/Assignment_4/dataset.json'))\n",
        "\n",
        "reviews[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNB_5wK3fbbD",
        "outputId": "4d8b89c9-8782-48fc-d2ad-18edeb9c0ffb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'apparel': 1717,\n",
              "         'baby': 1107,\n",
              "         'beauty': 993,\n",
              "         'books': 921,\n",
              "         'camera_&_photo': 1086,\n",
              "         'cell_phones_&_service': 698,\n",
              "         'dvd': 893,\n",
              "         'electronics': 1277,\n",
              "         'grocery': 1100,\n",
              "         'health_&_personal_care': 1429,\n",
              "         'jewelry_&_watches': 1086,\n",
              "         'kitchen_&_housewares': 1390,\n",
              "         'magazines': 1133,\n",
              "         'music': 1007,\n",
              "         'outdoor_living': 980,\n",
              "         'software': 1029,\n",
              "         'sports_&_outdoors': 1336,\n",
              "         'toys_&_games': 1363,\n",
              "         'video': 1010,\n",
              "         'automotive': 100,\n",
              "         'computer_&_video_games': 100,\n",
              "         'office_products': 100})"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "from collections import Counter\n",
        "mention_domain = [r['domain'] for r in reviews]\n",
        "Counter(mention_domain)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8sZDjijRgLhf"
      },
      "source": [
        "## Creating Meta Learning Tasks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "0e07FAdegEf3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import collections\n",
        "import random\n",
        "import json, pickle\n",
        "from torch.utils.data import TensorDataset\n",
        "\n",
        "LABEL_MAP  = {'positive':0, 'negative':1, 0:'positive', 1:'negative'}\n",
        "\n",
        "class MetaTask(Dataset):\n",
        "    \n",
        "    def __init__(self, examples, num_task, k_support, k_query, tokenizer):\n",
        "        \"\"\"\n",
        "        :param samples: list of samples\n",
        "        :param num_task: number of training tasks.\n",
        "        :param k_support: number of support sample per task\n",
        "        :param k_query: number of query sample per task\n",
        "        \"\"\"\n",
        "        self.examples = examples\n",
        "        random.shuffle(self.examples)\n",
        "        \n",
        "        self.num_task = num_task\n",
        "        self.k_support = k_support\n",
        "        self.k_query = k_query\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_seq_length = 128\n",
        "        self.create_batch(self.num_task)\n",
        "    \n",
        "    def create_batch(self, num_task):\n",
        "        self.supports = []  # support set\n",
        "        self.queries = []  # query set\n",
        "        for b in range(num_task):  # for each task\n",
        "            # 1.select domain randomly\n",
        "            domain = random.choice(self.examples)['domain']\n",
        "            domainExamples = [e for e in self.examples if e['domain'] == domain]\n",
        "            \n",
        "            # 1.select k_support + k_query examples from domain randomly\n",
        "            selected_examples = random.sample(domainExamples,self.k_support + self.k_query)\n",
        "            random.shuffle(selected_examples)\n",
        "            exam_train = selected_examples[:self.k_support]\n",
        "            exam_test  = selected_examples[self.k_support:]\n",
        "            \n",
        "            self.supports.append(exam_train)\n",
        "            self.queries.append(exam_test)\n",
        "    def create_feature_set(self,examples):\n",
        "        all_input_ids      = torch.empty(len(examples), self.max_seq_length, dtype = torch.long)\n",
        "        all_attention_mask = torch.empty(len(examples), self.max_seq_length, dtype = torch.long)\n",
        "        all_segment_ids    = torch.empty(len(examples), self.max_seq_length, dtype = torch.long)\n",
        "        all_label_ids      = torch.empty(len(examples), dtype = torch.long)\n",
        "\n",
        "        for id_,example in enumerate(examples):\n",
        "            input_ids = tokenizer.encode(example['text'])\n",
        "            attention_mask = [1] * len(input_ids)\n",
        "            segment_ids    = [0] * len(input_ids)\n",
        "\n",
        "            while len(input_ids) < self.max_seq_length:\n",
        "                input_ids.append(0)\n",
        "                attention_mask.append(0)\n",
        "                segment_ids.append(0)\n",
        "\n",
        "            label_id = LABEL_MAP[example['label']]\n",
        "            all_input_ids[id_] = torch.Tensor(input_ids).to(torch.long)\n",
        "            all_attention_mask[id_] = torch.Tensor(attention_mask).to(torch.long)\n",
        "            all_segment_ids[id_] = torch.Tensor(segment_ids).to(torch.long)\n",
        "            all_label_ids[id_] = torch.Tensor([label_id]).to(torch.long)\n",
        "\n",
        "        tensor_set = TensorDataset(all_input_ids, all_attention_mask, all_segment_ids, all_label_ids)  \n",
        "        return tensor_set\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        support_set = self.create_feature_set(self.supports[index])\n",
        "        query_set   = self.create_feature_set(self.queries[index])\n",
        "        return support_set, query_set\n",
        "\n",
        "    def __len__(self):\n",
        "        # as we have built up to batchsz of sets, you can sample some small batch size of sets.\n",
        "        return self.num_task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3N53MRZpgndg"
      },
      "source": [
        "## Split meta training and meta testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5eAaDwBgkLJ",
        "outputId": "70713901-43dd-4608-cef5-1fb0b717dc66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21555 300\n"
          ]
        }
      ],
      "source": [
        "low_resource_domains = [\"office_products\", \"automotive\", \"computer_&_video_games\"]\n",
        "train_examples = [r for r in reviews if r['domain'] not in low_resource_domains]\n",
        "test_examples = [r for r in reviews if r['domain'] in low_resource_domains]\n",
        "print(len(train_examples), len(test_examples))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4WGG-FMGhGXP",
        "outputId": "36b5f975-c24e-4787-994d-ede74874b7f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers==2.5.1 in /usr/local/lib/python3.7/dist-packages (2.5.1)\n",
            "Requirement already satisfied: tokenizers==0.5.2 in /usr/local/lib/python3.7/dist-packages (from transformers==2.5.1) (0.5.2)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from transformers==2.5.1) (1.25.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==2.5.1) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==2.5.1) (3.8.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==2.5.1) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==2.5.1) (2.23.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from transformers==2.5.1) (0.1.97)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==2.5.1) (1.21.6)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==2.5.1) (0.0.53)\n",
            "Requirement already satisfied: s3transfer<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from boto3->transformers==2.5.1) (0.6.0)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->transformers==2.5.1) (1.0.1)\n",
            "Requirement already satisfied: botocore<1.29.0,>=1.28.1 in /usr/local/lib/python3.7/dist-packages (from boto3->transformers==2.5.1) (1.28.1)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.29.0,>=1.28.1->boto3->transformers==2.5.1) (2.8.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.29.0,>=1.28.1->boto3->transformers==2.5.1) (1.25.11)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.29.0,>=1.28.1->boto3->transformers==2.5.1) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.5.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.5.1) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==2.5.1) (2.10)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.5.1) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==2.5.1) (7.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==2.5.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "DwGTo_GkgdDq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertModel, BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)\n",
        "train = MetaTask(train_examples, num_task = 20, k_support=100, k_query=30, tokenizer = tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Take a glance at the first two samples from support set of 1st meta-task"
      ],
      "metadata": {
        "id": "gaxZ0V-JXdUf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ImUkGHJagv-1",
        "outputId": "ea3c13a1-cf18-4d92-bff2-ce8a96e60dc2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'text': '...buy it here and save $s!Shipping cost from the UK added $46.95 to the price AND a Foreign Debit Card Transaction fee of $2.51 for a waste of $49.46 that could have been spent getting additional models.Or get the download version and save even more as like someone else pointed out, the included manual is near useless and has not improved since version 4. Practical Poser 6 is still the best book to get if you are serious about doing Poser 6 development',\n",
              "  'label': 'negative',\n",
              "  'domain': 'software'},\n",
              " {'text': \"Looks nice... but beware!Installed Norton 360 on the 6 workstations in our small office. As soon as I did that our accounting system (Peachtree) started to lock up a few times a day. Even after turing off the firewall on all workstations the accounting system still locked up.I've now uninstalled Norton 360 on the server PC and that seems to have solved the problem.I wish now that I had just upgraded from Norton Internet Security 2005 to 2007... Never again.\",\n",
              "  'label': 'negative',\n",
              "  'domain': 'software'}]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "train.supports[0][:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kqje84fjhTXh",
        "outputId": "a57a4d30-6e62-48ae-e240-7d9e78451cb3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[  101,  1012,  1012,  1012,  4965,  2009,  2182,  1998,  3828,  1002,\n",
              "           1055,   999,  7829,  3465,  2013,  1996,  2866,  2794,  1002,  4805,\n",
              "           1012,  5345,  2000,  1996,  3976,  1998,  1037,  3097,  2139, 16313,\n",
              "           4003, 12598,  7408,  1997,  1002,  1016,  1012,  4868,  2005,  1037,\n",
              "           5949,  1997,  1002,  4749,  1012,  4805,  2008,  2071,  2031,  2042,\n",
              "           2985,  2893,  3176,  4275,  1012,  2030,  2131,  1996,  8816,  2544,\n",
              "           1998,  3828,  2130,  2062,  2004,  2066,  2619,  2842,  4197,  2041,\n",
              "           1010,  1996,  2443,  6410,  2003,  2379, 11809,  1998,  2038,  2025,\n",
              "           5301,  2144,  2544,  1018,  1012,  6742, 13382,  2099,  1020,  2003,\n",
              "           2145,  1996,  2190,  2338,  2000,  2131,  2065,  2017,  2024,  3809,\n",
              "           2055,  2725, 13382,  2099,  1020,  2458,   102,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0],\n",
              "         [  101,  3504,  3835,  1012,  1012,  1012,  2021,  2022,  8059,   999,\n",
              "           5361, 10770,  9475,  2006,  1996,  1020,  2573, 12516,  2015,  1999,\n",
              "           2256,  2235,  2436,  1012,  2004,  2574,  2004,  1045,  2106,  2008,\n",
              "           2256,  9529,  2291,  1006, 18237, 13334,  1007,  2318,  2000,  5843,\n",
              "           2039,  1037,  2261,  2335,  1037,  2154,  1012,  2130,  2044, 28639,\n",
              "           2125,  1996,  2543,  9628,  2006,  2035,  2573, 12516,  2015,  1996,\n",
              "           9529,  2291,  2145,  5299,  2039,  1012,  1045,  1005,  2310,  2085,\n",
              "           4895,  7076,  9080,  3709, 10770,  9475,  2006,  1996,  8241,  7473,\n",
              "           1998,  2008,  3849,  2000,  2031, 13332,  1996,  3291,  1012,  1045,\n",
              "           4299,  2085,  2008,  1045,  2018,  2074,  9725,  2013, 10770,  4274,\n",
              "           3036,  2384,  2000,  2289,  1012,  1012,  1012,  2196,  2153,  1012,\n",
              "            102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0]]),\n",
              " tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
              " tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
              " tensor([1, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "# Let take a look at the first two samples from support set\n",
        "train[0][0][:2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcihCXcvhY8P"
      },
      "source": [
        "## Training Meta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "EkWs0woyhVPC"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import logging\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.CRITICAL)\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "def random_seed(value):\n",
        "    torch.backends.cudnn.deterministic=True\n",
        "    torch.manual_seed(value)\n",
        "    torch.cuda.manual_seed(value)\n",
        "    np.random.seed(value)\n",
        "    random.seed(value)\n",
        "\n",
        "def create_batch_of_tasks(taskset, is_shuffle = True, batch_size = 4):\n",
        "    idxs = list(range(0,len(taskset)))\n",
        "    if is_shuffle:\n",
        "        random.shuffle(idxs)\n",
        "    for i in range(0,len(idxs), batch_size):\n",
        "        yield [taskset[idxs[i]] for i in range(i, min(i + batch_size,len(taskset)))]\n",
        "\n",
        "class TrainingArgs:\n",
        "    def __init__(self):\n",
        "        self.num_labels = 2\n",
        "        self.meta_epoch=10\n",
        "        self.k_spt=80\n",
        "        self.k_qry=20\n",
        "        self.outer_batch_size = 2\n",
        "        self.inner_batch_size = 12\n",
        "        self.outer_update_lr = 5e-5\n",
        "        self.inner_update_lr = 5e-5\n",
        "        self.inner_update_step = 10\n",
        "        self.inner_update_step_eval = 40\n",
        "        self.bert_model = 'bert-base-uncased'\n",
        "        self.num_task_train = 10\n",
        "        self.num_task_test = 5\n",
        "\n",
        "args = TrainingArgs()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZ9vbEQnhgLr"
      },
      "source": [
        "Creating Meta Learner"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "_SQrUlUchcW3"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "from torch.optim import Adam\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from transformers import BertForSequenceClassification\n",
        "from copy import deepcopy\n",
        "import gc\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "class Learner(nn.Module):\n",
        "    \"\"\"\n",
        "    Meta Learner\n",
        "    \"\"\"\n",
        "    def __init__(self, args):\n",
        "        \"\"\"\n",
        "        :param args:\n",
        "        \"\"\"\n",
        "        super(Learner, self).__init__()\n",
        "        \n",
        "        self.num_labels = args.num_labels\n",
        "        self.outer_batch_size = args.outer_batch_size\n",
        "        self.inner_batch_size = args.inner_batch_size\n",
        "        self.outer_update_lr  = args.outer_update_lr\n",
        "        self.inner_update_lr  = args.inner_update_lr\n",
        "        self.inner_update_step = args.inner_update_step\n",
        "        self.inner_update_step_eval = args.inner_update_step_eval\n",
        "        self.bert_model = args.bert_model\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        \n",
        "        self.model = BertForSequenceClassification.from_pretrained(self.bert_model, num_labels = self.num_labels)\n",
        "        self.outer_optimizer = Adam(self.model.parameters(), lr=self.outer_update_lr)\n",
        "        self.model.train()\n",
        "    def forward(self, batch_tasks, training = True):\n",
        "        \"\"\"\n",
        "        batch = [(support TensorDataset, query TensorDataset),\n",
        "                 (support TensorDataset, query TensorDataset),\n",
        "                 (support TensorDataset, query TensorDataset),\n",
        "                 (support TensorDataset, query TensorDataset)]\n",
        "        \n",
        "        # support = TensorDataset(all_input_ids, all_attention_mask, all_segment_ids, all_label_ids)\n",
        "        \"\"\"\n",
        "        task_accs = []\n",
        "        sum_gradients = []\n",
        "        num_task = len(batch_tasks)\n",
        "        num_inner_update_step = self.inner_update_step if training else self.inner_update_step_eval\n",
        "\n",
        "        for task_id, task in enumerate(batch_tasks):\n",
        "            support = task[0]\n",
        "            query   = task[1]\n",
        "            \n",
        "            fast_model = deepcopy(self.model)\n",
        "            fast_model.to(self.device)\n",
        "            support_dataloader = DataLoader(support, sampler=RandomSampler(support),\n",
        "                                            batch_size=self.inner_batch_size)\n",
        "            \n",
        "            inner_optimizer = Adam(fast_model.parameters(), lr=self.inner_update_lr)\n",
        "            fast_model.train()\n",
        "            \n",
        "            print('----Task',task_id, '----')\n",
        "            for i in range(0,num_inner_update_step):\n",
        "                all_loss = []\n",
        "                for inner_step, batch in enumerate(support_dataloader):\n",
        "                    batch = tuple(t.to(self.device) for t in batch)\n",
        "                    input_ids, attention_mask, segment_ids, label_id = batch\n",
        "                    outputs = fast_model(input_ids, attention_mask, segment_ids, labels = label_id)\n",
        "                    \n",
        "                    loss = outputs[0]              \n",
        "                    loss.backward()\n",
        "                    inner_optimizer.step()\n",
        "                    inner_optimizer.zero_grad()\n",
        "                    \n",
        "                    all_loss.append(loss.item())\n",
        "                \n",
        "                if i % 4 == 0:\n",
        "                    print(\"Inner Loss: \", np.mean(all_loss))\n",
        "            \n",
        "            fast_model.to(torch.device('cpu'))\n",
        "            \n",
        "            if training:\n",
        "                meta_weights = list(self.model.parameters())\n",
        "                fast_weights = list(fast_model.parameters())\n",
        "\n",
        "                gradients = []\n",
        "                for i, (meta_params, fast_params) in enumerate(zip(meta_weights, fast_weights)):\n",
        "                    gradient = meta_params - fast_params\n",
        "                    if task_id == 0:\n",
        "                        sum_gradients.append(gradient)\n",
        "                    else:\n",
        "                        sum_gradients[i] += gradient\n",
        "            fast_model.to(self.device)\n",
        "            fast_model.eval()\n",
        "            with torch.no_grad():\n",
        "                query_dataloader = DataLoader(query, sampler=None, batch_size=len(query))\n",
        "                query_batch = iter(query_dataloader).next()\n",
        "                query_batch = tuple(t.to(self.device) for t in query_batch)\n",
        "                q_input_ids, q_attention_mask, q_segment_ids, q_label_id = query_batch\n",
        "                q_outputs = fast_model(q_input_ids, q_attention_mask, q_segment_ids, labels = q_label_id)\n",
        "\n",
        "                q_logits = F.softmax(q_outputs[1],dim=1)\n",
        "                pre_label_id = torch.argmax(q_logits,dim=1)\n",
        "                pre_label_id = pre_label_id.detach().cpu().numpy().tolist()\n",
        "                q_label_id = q_label_id.detach().cpu().numpy().tolist()\n",
        "\n",
        "                acc = accuracy_score(pre_label_id,q_label_id)\n",
        "                task_accs.append(acc)\n",
        "            \n",
        "            fast_model.to(torch.device('cpu'))\n",
        "            del fast_model, inner_optimizer\n",
        "            torch.cuda.empty_cache()\n",
        "        if training:\n",
        "            # Average gradient across tasks\n",
        "            for i in range(0,len(sum_gradients)):\n",
        "                sum_gradients[i] = sum_gradients[i] / float(num_task)\n",
        "\n",
        "            #Assign gradient for original model, then using optimizer to update its weights\n",
        "            for i, params in enumerate(self.model.parameters()):\n",
        "                params.grad = sum_gradients[i]\n",
        "\n",
        "            self.outer_optimizer.step()\n",
        "            self.outer_optimizer.zero_grad()\n",
        "            \n",
        "            del sum_gradients\n",
        "            gc.collect()\n",
        "        \n",
        "        return np.mean(task_accs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "Tz20uUNPhwaU"
      },
      "outputs": [],
      "source": [
        "learner = Learner(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "sHRuen8qhyXo"
      },
      "outputs": [],
      "source": [
        "random_seed(123)\n",
        "test = MetaTask(test_examples, num_task = 3, k_support=80, k_query=20, tokenizer = tokenizer)\n",
        "random_seed(int(time.time() % 10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-FhglAZh9dE",
        "outputId": "bb29b817-fb7b-4f89-f229-ff9f7c1c3945"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'text': 'Love love love the moleskin notebooks in every size. So cool looking and just get cooler as they get beat up. My favorite size is the small one because I can carry it around in my bag so that I can scribble down any random thoughts throughout the day. One of my unexpectedly best purchases!',\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'},\n",
              " {'text': \"A friend of mine had this planner and when I saw it I knew I just had to have it! It's great because I can keep track of my week on one side and then make my 'to do' lists or keep notes on the otherside of the page. I recommend this planner to EVERYONE\",\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'},\n",
              " {'text': 'Does it make any sense to purchase paper for 30.99 and then pay 27.99 for shipping to recieve it. Someone over there should really look at how realistic that is',\n",
              "  'label': 'negative',\n",
              "  'domain': 'office_products'},\n",
              " {'text': 'The nib on this pen is far too broad. Most fountain pen manufacturers would probably rate it \"medium\".It does write a nice, smooth line, and it is conveniently disposable, but it also looks very disposable. Its appearance is less elegant even than most rollerball pens.If you are considering buying this pen because you like the elegance of fountain pens, or for its \"fine\" point, then this is not the pen for you. These were both major considerations for me, and I was quite disappointed',\n",
              "  'label': 'negative',\n",
              "  'domain': 'office_products'},\n",
              " {'text': \"Keeps my wife's and my laptop running cool without any fans. My comp internal fan rarely runs when using Xpad\",\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'},\n",
              " {'text': 'I must admit I had no idea what they were, but my daughter wanted them. I was disappointed that all you do is basically rub the pins together to get another color. This color did not last long so you had to keep doing that little process to get the colors to blend. My daughter who is 7 lost interest quickly. She uses them like regular markers. I could have paid 5 dollars for a new pack of markers. As for the material to use them on, she used them all in a couple of hours.',\n",
              "  'label': 'negative',\n",
              "  'domain': 'office_products'},\n",
              " {'text': \"I've dealt with all other kinds of pocket notebooks, all of which fold, deform, get pages easily folded over and ripped out and stained. The Moleskine is an elegant little solution that protects itself with a thin, unobtrusive cover and a small elastic strap which has avoided every other problem that's plagued previous notebooks. I also like the price\",\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'},\n",
              " {'text': \"This wire stand will hold large books easily and will allow you to turn the pages with ease. I should have bought two of them.Excellent product and it's cheap\",\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'},\n",
              " {'text': 'i AM STILL WAITING FOR DELIVERY. pLEASE CONTACT YOUR VENDOR OR CANCEL ORDER. DO NOT CHARGE MY ACCOUNT.',\n",
              "  'label': 'negative',\n",
              "  'domain': 'office_products'},\n",
              " {'text': 'This product is excellent. It will make it much easier on your back and also time efficient.',\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'},\n",
              " {'text': \"I don't know why they changed from the old layout to the new skinny vertical columns. The old planner was a lifesaver, but this new layout is unusable. They claim to be the artist's notebook, but what artist is really going to be breaking their day down hour by hour? And even if someone wanted to do that, the tiny columns are worthless. Really a shame\",\n",
              "  'label': 'negative',\n",
              "  'domain': 'office_products'},\n",
              " {'text': 'I bought this pen recently and I really like the attractiveness of the pen. I believe this goes well for either a man or a woman. The pen writes very smoothly and offers you a quality pen at a reasonable price',\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'},\n",
              " {'text': 'I like these notebooks very much. The cover is very sturdy and the pages have a nice weight to them. The size is perfect for your back pocket or jacket. I use these books to jot down notes from meetings with clients. The small pocket in the rear of the book is handy for storing business cards',\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'},\n",
              " {'text': \"I've dealt with all other kinds of pocket notebooks, all of which fold, deform, get pages easily folded over and ripped out and stained. The Moleskine is an elegant little solution that protects itself with a thin, unobtrusive cover and a small elastic strap which has avoided every other problem that's plagued previous notebooks. I also like the price\",\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'},\n",
              " {'text': 'This is a great address book, but you should know that the price coming out of the factory is $9.95, not the higher prices cited in this listing',\n",
              "  'label': 'negative',\n",
              "  'domain': 'office_products'},\n",
              " {'text': 'This is a nice chair but has a very low weight limit to be classified a big and tall chair. Very misleading. I personally do not consider 250 lbs as big as that is my ideal weight. So buyer beware',\n",
              "  'label': 'negative',\n",
              "  'domain': 'office_products'},\n",
              " {'text': \"I like the size and look of the planner. The format is great too with days of the week on the left side and lined pages ont the right side. It's lightweight and you can carry it with you almost everywhere...even in a purse.But the paper is too thin. If I use any pen I can see what I have written on the other side. Not worth the amount I paid for with shipping and handling. Better to go to a store and see and feel your planner and then decide if you want to purchase it\",\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'},\n",
              " {'text': \"The Mokeskine notebooks are must haves for art majors. They're handy for quick sketches, laying out ideas for 2D/3D design class, keeping track of glazing formulas, class assignments, personal notes and reminders, email addresses and phone numbers, just about anything you can think of to draw or write down. The smaller ones fit in a pocket or small purse and the larger ones go nicely in your portfolio or backpack.\",\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'},\n",
              " {'text': \"There's a reason the price on this is low ... because it's a worthless piece of merchandise. I gave mine to good will two days after it came.\",\n",
              "  'label': 'negative',\n",
              "  'domain': 'office_products'},\n",
              " {'text': \"Without question the greatest journal I've held, including those priced much higher. I travel a lot, and find the notebooks hold-up in all conditions. Just owning it makes me want to write\",\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'},\n",
              " {'text': \"I was looking for a nice heatshield that did not use USB or drain energy from my laptop battery. This thing works great! I wouldn't hesitate to buy this\",\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'},\n",
              " {'text': 'This is the best handbag-size address book. Discreet, sexy, all grown up. I bought my first one in Italy and will continue to buy more (because I never write in pencil!',\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'},\n",
              " {'text': 'BEWARE!!! DESPITE THE PICTURE AND THE PRODUCT DESCRIPTION, THIS IS NOT A FOUNTAIN PEN. IT IS INDEED A BALLPOINT PEN AS THE PRODUCT TITLE INDICATES',\n",
              "  'label': 'negative',\n",
              "  'domain': 'office_products'},\n",
              " {'text': \"This chair is very comfortable, but I don't think the leather is real. The seat ripped before one month was up from purchase. It was easy to assemble, but not worth the money.\",\n",
              "  'label': 'negative',\n",
              "  'domain': 'office_products'},\n",
              " {'text': 'This notebook is fabulous...its history inspires. I think that it will be the last type of notebook I use for my journaling and writing. I adore it',\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'},\n",
              " {'text': 'Becareful it is a ballpoint pen but the picture was misleading. so have to return it',\n",
              "  'label': 'negative',\n",
              "  'domain': 'office_products'},\n",
              " {'text': \"I have owned this chair for about 9 months. The seat bottom has flattened out and has torn a bit -- I can see the polyfill inside. It is not particularly comfortable anymore. The chair also creaks a lot (as is typical with most inexpensive chairs). It's a lot of money for a chair that has the build quality of one of the cheapie leather chairs that can be had for under $50\",\n",
              "  'label': 'negative',\n",
              "  'domain': 'office_products'},\n",
              " {'text': 'Okay, you\\'re thinking, \"Ten bucks is a lot of money for a pocket notebook.\" Well, if you\\'re the kind who can survive with a lined pocket memo pad, this notebook\\'s not for you. If, however, you need a durable, elegant, pocket-sized way to record your ideas, observations, sketches, and anything else you can put on paper, this is it.',\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'},\n",
              " {'text': \"Anyone even remotely familiar with the Moleskine books know that they are the standard by which others are looked down upon. My only advice is to not buy them here unless you're not spending your own money--you'll pay the same price at any local stationer / hip-urban-gen-X shop, but won't get raked over the shipping coals like I foolishly was\",\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'},\n",
              " {'text': 'Okay, you\\'re thinking, \"Ten bucks is a lot of money for a pocket notebook.\" Well, if you\\'re the kind who can survive with a lined pocket memo pad, this notebook\\'s not for you. If, however, you need a durable, elegant, pocket-sized way to record your ideas, observations, sketches, and anything else you can put on paper, this is it.',\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'},\n",
              " {'text': \"I've dealt with all other kinds of pocket notebooks, all of which fold, deform, get pages easily folded over and ripped out and stained. The Moleskine is an elegant little solution that protects itself with a thin, unobtrusive cover and a small elastic strap which has avoided every other problem that's plagued previous notebooks. I would give it 5 stars if it was a little less expensive\",\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'},\n",
              " {'text': 'Over the last 10+ yrs. I have owned 4 laptops, one burnt my leg, and the others overheated. This marvelous device lets your laptop breathe. If you own a laptop, this is a must!!',\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'},\n",
              " {'text': \"I've dealt with all other kinds of pocket notebooks, all of which fold, deform, get pages easily folded over and ripped out and stained. The Moleskine is an elegant little solution that protects itself with a thin, unobtrusive cover and a small elastic strap which has avoided every other problem that's plagued previous notebooks. I would give it 5 stars if it was a little less expensive\",\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'},\n",
              " {'text': \"Anyone even remotely familiar with the Moleskine books know that they are the standard by which others are looked down upon. My only advice is to not buy them here unless you're not spending your own money--you'll pay the same price at any local stationer / hip-urban-gen-X shop, but won't get raked over the shipping coals like I foolishly was\",\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'},\n",
              " {'text': \"I've been using Cross pens since I returned to my fountain pen roots several years ago but I decided to try this model because of its gorgeous design. There are aspects of it that I like better. The cartridges are longer, unlike the smaller Cross ones that run out much quicker and never seem to drain completely. It was also ready to go as soon as the cartridge was inserted unlike the Cross which seem to be stubborn to get started. It has a nice feel to it ergonomically and I nice feel to the nib on the paper.\",\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'},\n",
              " {'text': \"Other than the fact that it turns on, this is a very cheaply made, cheap looking, cheap feeling product. The bottom battery cover doesn't even connect properly and feels as if it will pop off at any moment. Did I mention CHEAP?!?\",\n",
              "  'label': 'negative',\n",
              "  'domain': 'office_products'},\n",
              " {'text': \"This is a great quality product. Best quality transfer paper I've ever used. It's pretty expensive if you buy it here though. You can get this same exact product at your local Office Depot for a cheaper price plus you don't have to pay shipping/handling\",\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'},\n",
              " {'text': 'I had purchased a Levenger palm pilot and writing pad case, which came with loops for a pen and pencil, but did not come with any appropriately sized for the compact case. Therefore, I looked online for mini pens that were stylish, compact enough for my case, inexpensive, and could be refilled. These definitely meet all of my criteria',\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'},\n",
              " {'text': 'These books are a good idea for the repeat visitor to a city-- basically just a place for notes in the lovely (I think) Moleskine format. The maps, though, can be found better and cheaper on other places. MORE IMPORTANT, this seller has claimed a falsely inflated list price and consequent discount that is misleading. The list price for these books is 17.95. Check in any book store. Shame on the seller',\n",
              "  'label': 'negative',\n",
              "  'domain': 'office_products'},\n",
              " {'text': 'Every time I write on it holes go right through it! Dont by this! If you fold it, it rips in the fold!! Dont Buy this!!! You will be sorry',\n",
              "  'label': 'negative',\n",
              "  'domain': 'office_products'},\n",
              " {'text': \"Poor communication. I was notified that the printer was not coming when they said it would, but was never given another date. After waiting some time I cancelled the order and said I ordered another one from someone else. I guess they don't read their email either. The next day they shipped the item. Now I have two, but am not going to spend another $20 to ship it back to them. Believe me I would never buy from this company again\",\n",
              "  'label': 'negative',\n",
              "  'domain': 'office_products'},\n",
              " {'text': \"Without question the greatest journal I've held, including those priced much higher. I travel a lot, and find the notebooks hold-up in all conditions. Just owning it makes me want to write\",\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'},\n",
              " {'text': \"The brushes and everything else are super tiny, only a child's hand can hold those. Not a good set at all. Rather just go to micheal's art store and get the sumi set there. Just stay away from this set, unless you you buy this as a gift for your 3 years old child\",\n",
              "  'label': 'negative',\n",
              "  'domain': 'office_products'},\n",
              " {'text': 'I like this cooler/heatshield much better then the one I had previously. It keeps the laptop much cooler and is easier to transport when I am traveling',\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'},\n",
              " {'text': 'At first I thought it difficult to justify the added expense for paper, but you do get good quality. One reviewer comments on stains; but after buying and using 5 or 6 different moleskines, I have to believe that was an exception that would have been replaced.I like the Reporter Style for writing quickly or with limited space, but I like the bookmark that is on the standard notebooks',\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'},\n",
              " {'text': \"The Xpad is exactly as described ... non-slip surface, eliminates heat transfer between my legs and my laptop, elevates my laptop to allow proper airflow...and it fits easily into my laptop bag. The product is stable without being bulky or heavy and it doesn't interfere with my unit's internal cooling system\",\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'},\n",
              " {'text': \"These notebooks seem like a pricey investment but they're well worth it. I use mine for meetings, journaling, and sketching out ideas. Not only is it sturdy enough to take a beating, but it has a touch of class that a legal pad can't match.\",\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'},\n",
              " {'text': \"You will still need either a good book or a friend to help you figure out how to hold the pen and make the letters. This is just a kit w/ paper and pens and the pens weren't even that great. No matter how you write they don't make lines that transition between fat and thin. Might as well use a regular pen...I am kinda dissapointed and will keep looking for better pens. I give it 2 stars for dollar value.\",\n",
              "  'label': 'negative',\n",
              "  'domain': 'office_products'},\n",
              " {'text': \"I got this chair because I spend a lot of time on front of my home computer and I wanted a roomy chair that would allow me to move around some. I am tall, six feet, but not big, 127 pounds, and I have had this chair for less than six months and the leather is falling apart and the stuffing is coming out of the seat. I don't mind spending money to get a good chair but I cannot afford to be buying two chairs a year at this price.\",\n",
              "  'label': 'negative',\n",
              "  'domain': 'office_products'},\n",
              " {'text': \"Don't get me wrong -- this is a great product; however, $27.97 for shipping from Amazom or $7.95 shipping from Office Depot. Whatsup with that? Is Amazon tacking on a $20 handling fee? I ordered directly from Office Depot and now it makes me wonder how many of my other purchases from Amazon were the same\",\n",
              "  'label': 'negative',\n",
              "  'domain': 'office_products'},\n",
              " {'text': 'First, I\\'d just like to correct the person who said that moleskines have \"no actual historical legacy\". The Moleskine company may not have been founded until 1992, but moleskine notebooks have been around since before that. The company is not saying that Hemingway, Picasso and Chatwin used their brand of notebooks, they are saying that those people used moleskines.But the great thing about these notebooks is not that famous people used them, but that they are of great quality. I highly recommend them',\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'},\n",
              " {'text': \"I purchased this model at a local store after reading mixed reviews. It was in my price range and had at least some good reviews so I thought I'd take a chance. WOW, bad idea! After about 25 sheets of paper fed 1 page at a time the feed mechanism stopped functioning. I was careful not to overload the unit or run it too long, but the thing didn't last 5 minutes!!! Stay away\",\n",
              "  'label': 'negative',\n",
              "  'domain': 'office_products'},\n",
              " {'text': 'Really good qualty product !! I read all the time especially at the computer and this has been a great addition to my library and has increased my performance I Highly Recommend!!!!!!',\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'},\n",
              " {'text': 'I bought my first set of five sheets days ago, with no real idea with what i wanted to use it for. Just tonight I decided to spruce up my backpack for school using various images. It was very easy, and the results are amazing! I strongly suggest that everybody try it out, it really brought a new, more glamourous life to that old backpack!',\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'},\n",
              " {'text': 'These books are a good idea for the repeat visitor to a city-- basically just a place for notes in the lovely (I think) Moleskine format. The maps, though, can be found better and cheaper on other places. MORE IMPORTANT, this seller has claimed a falsely inflated list price and consequent discount that is misleading. The list price for these books is 17.95. Check in any book store. Shame on the seller',\n",
              "  'label': 'negative',\n",
              "  'domain': 'office_products'},\n",
              " {'text': \"This wire stand will hold large books easily and will allow you to turn the pages with ease. I should have bought two of them.Excellent product and it's cheap\",\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'},\n",
              " {'text': 'I use large ruled one as notebook for my arab language classes, large ruled one for work, small ruled one as vade mecum (always have it with me and use it to jot down EVERYTHING that comes to mind, addresses, phone numbers, PINs, moonphases, webpages to check, you name it), large blank paged one as BOS. I just LOVE it, its durable, elegant, compact.. and widely available in Croatia, thank God :',\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'},\n",
              " {'text': \"I would have to agree with the problem of the dividers to separate the dates, it just doesn't work. I didn't send it back because it really goes with my desk and I do like the two small drawers. Maybe I can fit some nice flowers or a decorative tissue box or something in the top bin and place mail somewhere else. I wouldn't have put my name on this thing. Does anyone out there need extra plastic dividers for this thing? You can have mine.\",\n",
              "  'label': 'negative',\n",
              "  'domain': 'office_products'},\n",
              " {'text': 'I use large ruled one as notebook for my arab language classes, large ruled one for work, small ruled one as vade mecum (always have it with me and use it to jot down EVERYTHING that comes to mind, addresses, phone numbers, PINs, moonphases, webpages to check, you name it), large blank paged one as BOS. I just LOVE it, its durable, elegant, compact.. and widely available in Croatia, thank God :',\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'},\n",
              " {'text': 'These books are a good idea for the repeat visitor to a city-- basically just a place for notes in the lovely (I think) Moleskine format. The maps, though, can be found better and cheaper on other places. MORE IMPORTANT, this seller has claimed a falsely inflated list price and consequent discount that is misleading. The list price for these books is 17.95. Check in any book store. Shame on the seller',\n",
              "  'label': 'negative',\n",
              "  'domain': 'office_products'},\n",
              " {'text': 'Love love love the moleskin notebooks in every size. So cool looking and just get cooler as they get beat up. My favorite size is the small one because I can carry it around in my bag so that I can scribble down any random thoughts throughout the day. One of my unexpectedly best purchases!',\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'},\n",
              " {'text': \"These notebooks seem like a pricey investment but they're well worth it. I use mine for meetings, journaling, and sketching out ideas. Not only is it sturdy enough to take a beating, but it has a touch of class that a legal pad can't match.\",\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'},\n",
              " {'text': 'this has got to be to most poorly manufactured item i have bought in a long time. Six of the plastic deviders were either cut too short so they wont fit into both groves spanning the opening, or were bowed in an arc shape. So my 31 day monthly organizer is now a 16 or 17 day organizer. You would think that someone would check the quality of something they are going to attach their name too and sell',\n",
              "  'label': 'negative',\n",
              "  'domain': 'office_products'},\n",
              " {'text': \"Nothing compares to these awesome notebooks. You'll never go back to the office supply variety after you've used one of these\",\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'},\n",
              " {'text': 'When one looks at the picture it shows a carton of paper. However, only one ream is sent. I was unaware of this. I got my one ream for 15 dollars after shipping and it was ripped to hell. Customer service wont answer and I guess I am just SOL with it',\n",
              "  'label': 'negative',\n",
              "  'domain': 'office_products'},\n",
              " {'text': \"Almost immediately after I received this never-used pen, I noticed that the ink does not dispense evenly. Often, it does not write on any surface. After I scratch the pen tip wildly for ~5 seconds, it starts to write again. I've thrown it away because it is too inconvenient for use. However, it is very cheap, so maybe it's okay for someone who doesn't need to use a tiny pen too often\",\n",
              "  'label': 'negative',\n",
              "  'domain': 'office_products'},\n",
              " {'text': 'Planner is nice, but I was charged $6.25 for shipping! Rip Off!!',\n",
              "  'label': 'negative',\n",
              "  'domain': 'office_products'},\n",
              " {'text': 'This heater died after a month of usage. I would rather go to Walmart and buy a cheap one (which I did) and it has lasted me for 2 months so far (with heavy usage). If that one breaks I can atleast drive down and return it Vs the hassle of Shipping it.',\n",
              "  'label': 'negative',\n",
              "  'domain': 'office_products'},\n",
              " {'text': 'top had to be on basket properly and paper would be on the floor had to keep vacume handy shredder ran only part of the time and kept jamming and finnaly stopped working had to return',\n",
              "  'label': 'negative',\n",
              "  'domain': 'office_products'},\n",
              " {'text': 'This notebook is fabulous...its history inspires. I think that it will be the last type of notebook I use for my journaling and writing. I adore it',\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'},\n",
              " {'text': 'Though lacking good directions, this was straightforward to assemble. However, it is flimsy and tilts whenever I lean on it ever so slightly. I am getting rid of it.',\n",
              "  'label': 'negative',\n",
              "  'domain': 'office_products'},\n",
              " {'text': 'This is a great note book. Is very well constructed and will sustain abuse. I am a civil engineer and use it on the field constantly I have 2 a small 3.5 by 5.5 and a medium. I particularly recommend the small with squares the size is perfect for you back pocket',\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'},\n",
              " {'text': 'Really good qualty product !! I read all the time especially at the computer and this has been a great addition to my library and has increased my performance I Highly Recommend!!!!!!',\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'},\n",
              " {'text': 'These books are a good idea for the repeat visitor to a city-- basically just a place for notes in the lovely (I think) Moleskine format. The maps, though, can be found better and cheaper on other places. MORE IMPORTANT, this seller has claimed a falsely inflated list price and consequent discount that is misleading. The list price for these books is 17.95. Check in any book store. Shame on the seller',\n",
              "  'label': 'negative',\n",
              "  'domain': 'office_products'},\n",
              " {'text': \"The X-Pad is a decent piece of equipment, doing a good job of keeping my laptop from getting too hot. My only issue with it is the lack of padding on the underside. I'm going to have to attach a piece of neoprene or something similar to make it more comfortable to have on my lap.\",\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'},\n",
              " {'text': 'This is a great note book. Is very well constructed and will sustain abuse. I am a civil engineer and use it on the field constantly I have 2 a small 3.5 by 5.5 and a medium. I particularly recommend the small with squares the size is perfect for you back pocket',\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'},\n",
              " {'text': 'At first I thought it difficult to justify the added expense for paper, but you do get good quality. One reviewer comments on stains; but after buying and using 5 or 6 different moleskines, I have to believe that was an exception that would have been replaced.I like the Reporter Style for writing quickly or with limited space, but I like the bookmark that is on the standard notebooks',\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'},\n",
              " {'text': 'This shipper (\"Moleskine Books\") really went above and beyond what I would have expected. They were pleasant to work with, quick to respond to questions, quick to ship, and really courteous from start to finish. Awesome product, awesome seller, I\\'m looking forward to ordering from them again',\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'},\n",
              " {'text': 'We had the Royal for about 8 months, used it sparingly and made sure not to overload it the it stopped working. We read reviews prior to purchase and it was recommended then to find other reviews now that support our experience. Stay away from this shredder.',\n",
              "  'label': 'negative',\n",
              "  'domain': 'office_products'},\n",
              " {'text': 'This shipper (\"Moleskine Books\") really went above and beyond what I would have expected. They were pleasant to work with, quick to respond to questions, quick to ship, and really courteous from start to finish. Awesome product, awesome seller, I\\'m looking forward to ordering from them again',\n",
              "  'label': 'positive',\n",
              "  'domain': 'office_products'}]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "test.supports[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLkhURm9iDVH"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TWOAI09oh_67",
        "outputId": "04125bd8-70f5-4f6e-9fee-297b0a3f21b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----Task 0 ----\n",
            "Inner Loss:  0.7173990692411151\n",
            "Inner Loss:  0.06377800021852766\n",
            "Inner Loss:  0.0037996030878275633\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.6026502336774554\n",
            "Inner Loss:  0.06383104808628559\n",
            "Inner Loss:  0.004119277931749821\n",
            "Step: 0 \ttraining Acc: 0.7\n",
            "\n",
            "-----------------Testing Mode-----------------\n",
            "\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.6777281420571464\n",
            "Inner Loss:  0.014927135647407599\n",
            "Inner Loss:  0.001665695570409298\n",
            "Inner Loss:  0.0009195762520123805\n",
            "Inner Loss:  0.0006575825391337276\n",
            "Inner Loss:  0.0004932242819839823\n",
            "Inner Loss:  0.00039933801079834145\n",
            "Inner Loss:  0.00034068627116669504\n",
            "Inner Loss:  0.0002873630770149508\n",
            "Inner Loss:  0.00024793007261385877\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.5497982374259404\n",
            "Inner Loss:  0.017872560104089125\n",
            "Inner Loss:  0.004533191677182913\n",
            "Inner Loss:  0.002481567324139178\n",
            "Inner Loss:  0.0015738967174131954\n",
            "Inner Loss:  0.0011426725790702871\n",
            "Inner Loss:  0.0008683831131617938\n",
            "Inner Loss:  0.0006934135370621723\n",
            "Inner Loss:  0.000567113988966282\n",
            "Inner Loss:  0.0004918949312663504\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.6210008689335415\n",
            "Inner Loss:  0.013570272829383612\n",
            "Inner Loss:  0.004174649316285338\n",
            "Inner Loss:  0.0015222214029303619\n",
            "Inner Loss:  0.0008164679914313767\n",
            "Inner Loss:  0.0005953499930910766\n",
            "Inner Loss:  0.0004742281245333808\n",
            "Inner Loss:  0.0003957668169667678\n",
            "Inner Loss:  0.00030371460681115944\n",
            "Inner Loss:  0.00028333078288207095\n",
            "Step: 0 Test F1: 0.8666666666666667\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.5441395895821708\n",
            "Inner Loss:  0.015149167073624474\n",
            "Inner Loss:  0.003291929074163948\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.5988989302090236\n",
            "Inner Loss:  0.01643724420240947\n",
            "Inner Loss:  0.001946441746050758\n",
            "Step: 1 \ttraining Acc: 0.9\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.5505816084997994\n",
            "Inner Loss:  0.013297279032745532\n",
            "Inner Loss:  0.014481072979313987\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.5959453540188926\n",
            "Inner Loss:  0.008387863303401641\n",
            "Inner Loss:  0.004396219205643449\n",
            "Step: 2 \ttraining Acc: 0.875\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.5250113947050912\n",
            "Inner Loss:  0.006436126240129981\n",
            "Inner Loss:  0.0014533670619130135\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.5383447025503431\n",
            "Inner Loss:  0.013886669783719949\n",
            "Inner Loss:  0.06002568766208632\n",
            "Step: 3 \ttraining Acc: 0.875\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.35065844655036926\n",
            "Inner Loss:  0.09833544333066259\n",
            "Inner Loss:  0.003951883043295571\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.3839207738637924\n",
            "Inner Loss:  0.1268291661648878\n",
            "Inner Loss:  0.003771256422623992\n",
            "Step: 4 \ttraining Acc: 0.8999999999999999\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.17092147416302136\n",
            "Inner Loss:  0.0039729355568332335\n",
            "Inner Loss:  0.001000914755942566\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.5670748821326664\n",
            "Inner Loss:  0.11755335942975112\n",
            "Inner Loss:  0.09978537607405867\n",
            "Step: 0 \ttraining Acc: 0.85\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.32860769970076426\n",
            "Inner Loss:  0.012511288481099265\n",
            "Inner Loss:  0.0025764452459822807\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.30266714308943066\n",
            "Inner Loss:  0.00623986530782921\n",
            "Inner Loss:  0.002156838558481208\n",
            "Step: 1 \ttraining Acc: 0.85\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.32390070600169046\n",
            "Inner Loss:  0.011500568661306585\n",
            "Inner Loss:  0.002326942315059049\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.5107611588069371\n",
            "Inner Loss:  0.007954346149095468\n",
            "Inner Loss:  0.002324100517268692\n",
            "Step: 2 \ttraining Acc: 0.9\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.34052163681813646\n",
            "Inner Loss:  0.015177040294344937\n",
            "Inner Loss:  0.0025657906995287965\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.23788095265626907\n",
            "Inner Loss:  0.01661096140742302\n",
            "Inner Loss:  0.0032135290426335166\n",
            "Step: 3 \ttraining Acc: 0.85\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.467457115650177\n",
            "Inner Loss:  0.008472001047006674\n",
            "Inner Loss:  0.001482529831784112\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.3281617116715227\n",
            "Inner Loss:  0.06375890650919505\n",
            "Inner Loss:  0.007134076141353164\n",
            "Step: 4 \ttraining Acc: 0.8999999999999999\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.288423591958625\n",
            "Inner Loss:  0.007081630174070597\n",
            "Inner Loss:  0.001389633198933942\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.3261145363960947\n",
            "Inner Loss:  0.030061069077679088\n",
            "Inner Loss:  0.002822481095790863\n",
            "Step: 0 \ttraining Acc: 0.8\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.2728489139782531\n",
            "Inner Loss:  0.015053722874394484\n",
            "Inner Loss:  0.006209501564236624\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.30219158051269396\n",
            "Inner Loss:  0.008031582393284355\n",
            "Inner Loss:  0.0015718963030459626\n",
            "Step: 1 \ttraining Acc: 0.8\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.18435016061578477\n",
            "Inner Loss:  0.020823458675295115\n",
            "Inner Loss:  0.001570015979398574\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.4287898444703647\n",
            "Inner Loss:  0.12765201314219407\n",
            "Inner Loss:  0.006482017692178488\n",
            "Step: 2 \ttraining Acc: 0.925\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.35214984230697155\n",
            "Inner Loss:  0.008205974541072334\n",
            "Inner Loss:  0.0018310020178822534\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.3051409034856728\n",
            "Inner Loss:  0.00654652518486338\n",
            "Inner Loss:  0.0017486175056546926\n",
            "Step: 3 \ttraining Acc: 0.875\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.374792871464576\n",
            "Inner Loss:  0.005871650097625596\n",
            "Inner Loss:  0.0015297730153958713\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.406767447079931\n",
            "Inner Loss:  0.016605674555259093\n",
            "Inner Loss:  0.0023547628495310035\n",
            "Step: 4 \ttraining Acc: 0.875\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.34965017278279575\n",
            "Inner Loss:  0.04167817905545235\n",
            "Inner Loss:  0.002838383595060025\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.25172621943056583\n",
            "Inner Loss:  0.01107144049767937\n",
            "Inner Loss:  0.0037948124310267822\n",
            "Step: 0 \ttraining Acc: 0.825\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.4190659201038735\n",
            "Inner Loss:  0.01674546461020197\n",
            "Inner Loss:  0.002762965558628951\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.5481078375929168\n",
            "Inner Loss:  0.15181994012423924\n",
            "Inner Loss:  0.005956781928294471\n",
            "Step: 1 \ttraining Acc: 0.95\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.3488364459148475\n",
            "Inner Loss:  0.0641624741256237\n",
            "Inner Loss:  0.004316703782283834\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.3973776600988848\n",
            "Inner Loss:  0.02585185132920742\n",
            "Inner Loss:  0.004557194387806314\n",
            "Step: 2 \ttraining Acc: 0.9\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.3117333783635071\n",
            "Inner Loss:  0.05521215285573687\n",
            "Inner Loss:  0.006483090376215321\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.35857899992593695\n",
            "Inner Loss:  0.02769127009170396\n",
            "Inner Loss:  0.0034908225892909934\n",
            "Step: 3 \ttraining Acc: 0.95\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.19442833587527275\n",
            "Inner Loss:  0.005691681123737778\n",
            "Inner Loss:  0.0013061514224058815\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.32398005701335414\n",
            "Inner Loss:  0.010449231336159366\n",
            "Inner Loss:  0.002073232367235635\n",
            "Step: 4 \ttraining Acc: 0.9\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.3048524240231408\n",
            "Inner Loss:  0.00992839244593467\n",
            "Inner Loss:  0.002541252179071307\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.2821078360346811\n",
            "Inner Loss:  0.09332723862358502\n",
            "Inner Loss:  0.019105970061251094\n",
            "Step: 0 \ttraining Acc: 0.875\n",
            "\n",
            "-----------------Testing Mode-----------------\n",
            "\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.3719271089648828\n",
            "Inner Loss:  0.06478909376476492\n",
            "Inner Loss:  0.02412907113986356\n",
            "Inner Loss:  0.0044562323684138915\n",
            "Inner Loss:  0.002104714047163725\n",
            "Inner Loss:  0.0013698447063299163\n",
            "Inner Loss:  0.0009716811889250364\n",
            "Inner Loss:  0.0007359012379311025\n",
            "Inner Loss:  0.0006178722750129444\n",
            "Inner Loss:  0.000505531023788665\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.5955546920054725\n",
            "Inner Loss:  0.033162977546453476\n",
            "Inner Loss:  0.005069818214646408\n",
            "Inner Loss:  0.0021475741107548985\n",
            "Inner Loss:  0.0012158722716516682\n",
            "Inner Loss:  0.0008160095728401627\n",
            "Inner Loss:  0.0005961459163310272\n",
            "Inner Loss:  0.00046778563825812725\n",
            "Inner Loss:  0.0003729184973053634\n",
            "Inner Loss:  0.00031339487343627425\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.38258682949734585\n",
            "Inner Loss:  0.012484185597194093\n",
            "Inner Loss:  0.0025694131784673247\n",
            "Inner Loss:  0.001384951390459069\n",
            "Inner Loss:  0.0008489893849140831\n",
            "Inner Loss:  0.0005963699659332633\n",
            "Inner Loss:  0.0004630789834274245\n",
            "Inner Loss:  0.00037530894873530736\n",
            "Inner Loss:  0.0002654564928629303\n",
            "Inner Loss:  0.00025023616035468876\n",
            "Step: 0 Test F1: 0.9666666666666667\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.422147617675364\n",
            "Inner Loss:  0.07499700279108115\n",
            "Inner Loss:  0.0037751129628824337\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.6253389811941555\n",
            "Inner Loss:  0.06503431419176715\n",
            "Inner Loss:  0.0515564849733242\n",
            "Step: 1 \ttraining Acc: 0.95\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.6560109564369279\n",
            "Inner Loss:  0.04595794927861009\n",
            "Inner Loss:  0.006338088174483606\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.24529502873442002\n",
            "Inner Loss:  0.019390713955674852\n",
            "Inner Loss:  0.0023046358754592283\n",
            "Step: 2 \ttraining Acc: 0.95\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.36811931185158236\n",
            "Inner Loss:  0.019548423854368075\n",
            "Inner Loss:  0.0034490436914243867\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.5712657912767359\n",
            "Inner Loss:  0.026135667360254695\n",
            "Inner Loss:  0.0061972543065037045\n",
            "Step: 3 \ttraining Acc: 0.875\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.3046269716994305\n",
            "Inner Loss:  0.06635095977357455\n",
            "Inner Loss:  0.009353928134909697\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.45098731339177384\n",
            "Inner Loss:  0.01817954797297716\n",
            "Inner Loss:  0.0031097474774079664\n",
            "Step: 4 \ttraining Acc: 0.925\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.36572853643779774\n",
            "Inner Loss:  0.09212277191025871\n",
            "Inner Loss:  0.004442439913483602\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.27169053895132883\n",
            "Inner Loss:  0.005628533528319427\n",
            "Inner Loss:  0.0013288356297250306\n",
            "Step: 0 \ttraining Acc: 0.925\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.5416311814582774\n",
            "Inner Loss:  0.02239011800182717\n",
            "Inner Loss:  0.002941077674872109\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.5369164538569748\n",
            "Inner Loss:  0.11889137274452619\n",
            "Inner Loss:  0.0038469844896878514\n",
            "Step: 1 \ttraining Acc: 0.875\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.21313429672071443\n",
            "Inner Loss:  0.0759005392236369\n",
            "Inner Loss:  0.04513588408008218\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.26615432842767667\n",
            "Inner Loss:  0.005115993520511048\n",
            "Inner Loss:  0.0012059757386201195\n",
            "Step: 2 \ttraining Acc: 0.925\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.559607173715319\n",
            "Inner Loss:  0.08627216331660748\n",
            "Inner Loss:  0.006587116779493434\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.6398277857473919\n",
            "Inner Loss:  0.014337782748043537\n",
            "Inner Loss:  0.004172150012371796\n",
            "Step: 3 \ttraining Acc: 0.925\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.22868194103024767\n",
            "Inner Loss:  0.009837657612349306\n",
            "Inner Loss:  0.0017646713781037501\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.46133974430683466\n",
            "Inner Loss:  0.01342740428767034\n",
            "Inner Loss:  0.0029970761887463076\n",
            "Step: 4 \ttraining Acc: 0.825\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.283331451471895\n",
            "Inner Loss:  0.007922692690044641\n",
            "Inner Loss:  0.0018558487562196596\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.5294011847249099\n",
            "Inner Loss:  0.027920920667903765\n",
            "Inner Loss:  0.0031995258094476803\n",
            "Step: 0 \ttraining Acc: 0.875\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.3302474185974071\n",
            "Inner Loss:  0.02508134288447244\n",
            "Inner Loss:  0.005453494710049459\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.4603198340960911\n",
            "Inner Loss:  0.050704884475895336\n",
            "Inner Loss:  0.003717040194065443\n",
            "Step: 1 \ttraining Acc: 0.875\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.38731136377568226\n",
            "Inner Loss:  0.009624674716698272\n",
            "Inner Loss:  0.0029560268989631106\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.5441309563549501\n",
            "Inner Loss:  0.013619624344365937\n",
            "Inner Loss:  0.0034011166343199356\n",
            "Step: 2 \ttraining Acc: 1.0\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.39279562073560165\n",
            "Inner Loss:  0.014875064071800028\n",
            "Inner Loss:  0.002660995249503425\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.1727305218643908\n",
            "Inner Loss:  0.0028997663674610002\n",
            "Inner Loss:  0.0010180914458552642\n",
            "Step: 3 \ttraining Acc: 0.8500000000000001\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.3318485466630331\n",
            "Inner Loss:  0.039888439300869195\n",
            "Inner Loss:  0.0024504465982317924\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.26466441517030553\n",
            "Inner Loss:  0.022143841721117496\n",
            "Inner Loss:  0.0020306036874119726\n",
            "Step: 4 \ttraining Acc: 0.975\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.31618916070354836\n",
            "Inner Loss:  0.06321209762245417\n",
            "Inner Loss:  0.005353139414052878\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.4076165168413094\n",
            "Inner Loss:  0.01749986476664032\n",
            "Inner Loss:  0.0020785986312798093\n",
            "Step: 0 \ttraining Acc: 0.8999999999999999\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.33512394634973525\n",
            "Inner Loss:  0.08474275363343102\n",
            "Inner Loss:  0.02942300694329398\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.2332389732390376\n",
            "Inner Loss:  0.0034429304235215697\n",
            "Inner Loss:  0.0006447141848704112\n",
            "Step: 1 \ttraining Acc: 0.95\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.20398741860740952\n",
            "Inner Loss:  0.022527837327548435\n",
            "Inner Loss:  0.005090934357472828\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.604750509207536\n",
            "Inner Loss:  0.06081363611987659\n",
            "Inner Loss:  0.006421503843739629\n",
            "Step: 2 \ttraining Acc: 0.975\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.3296529375615397\n",
            "Inner Loss:  0.009979312840316976\n",
            "Inner Loss:  0.0021447486942633986\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.4643136849959514\n",
            "Inner Loss:  0.00829748170716422\n",
            "Inner Loss:  0.0024689346984294908\n",
            "Step: 3 \ttraining Acc: 0.875\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.31520320840978194\n",
            "Inner Loss:  0.004386377188244036\n",
            "Inner Loss:  0.0007274027281839933\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.4101602842233011\n",
            "Inner Loss:  0.017875219828316143\n",
            "Inner Loss:  0.0025607057281636764\n",
            "Step: 4 \ttraining Acc: 0.925\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.37890657423330204\n",
            "Inner Loss:  0.03181496701602425\n",
            "Inner Loss:  0.0023535208477239522\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.3310108300065622\n",
            "Inner Loss:  0.008871531912258692\n",
            "Inner Loss:  0.001985947135835886\n",
            "Step: 0 \ttraining Acc: 0.95\n",
            "\n",
            "-----------------Testing Mode-----------------\n",
            "\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.325465528577167\n",
            "Inner Loss:  0.040359643287956715\n",
            "Inner Loss:  0.002058549351724131\n",
            "Inner Loss:  0.0007249304825173957\n",
            "Inner Loss:  0.00041575934821074564\n",
            "Inner Loss:  0.0002948923294232892\n",
            "Inner Loss:  0.00022426833415270915\n",
            "Inner Loss:  0.0001886293196418722\n",
            "Inner Loss:  0.00015243575658782253\n",
            "Inner Loss:  0.00013901740229422494\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.177540078858978\n",
            "Inner Loss:  0.0024720400771392243\n",
            "Inner Loss:  0.0007729260001464614\n",
            "Inner Loss:  0.00038473872908590626\n",
            "Inner Loss:  0.0001953716086323506\n",
            "Inner Loss:  0.00012872893213560537\n",
            "Inner Loss:  9.919312911474012e-05\n",
            "Inner Loss:  7.566913388602967e-05\n",
            "Inner Loss:  6.499043229268864e-05\n",
            "Inner Loss:  5.761849128508142e-05\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.24364037698666965\n",
            "Inner Loss:  0.002686661485183452\n",
            "Inner Loss:  0.0007422082708217204\n",
            "Inner Loss:  0.0003742306975514761\n",
            "Inner Loss:  0.00025338706161294667\n",
            "Inner Loss:  0.00017484878896669085\n",
            "Inner Loss:  0.00015017049320574318\n",
            "Inner Loss:  0.00012464038341672027\n",
            "Inner Loss:  9.592026097899569e-05\n",
            "Inner Loss:  8.791517965229494e-05\n",
            "Step: 0 Test F1: 0.9666666666666667\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.442681895835059\n",
            "Inner Loss:  0.006410514270620686\n",
            "Inner Loss:  0.002382421150936612\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.29101920946933596\n",
            "Inner Loss:  0.002707842970266938\n",
            "Inner Loss:  0.0009604107222652861\n",
            "Step: 1 \ttraining Acc: 0.925\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.19019869568624667\n",
            "Inner Loss:  0.007790724979713559\n",
            "Inner Loss:  0.0005436264369304159\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.3862658643296787\n",
            "Inner Loss:  0.007119287370837161\n",
            "Inner Loss:  0.0015264119221163647\n",
            "Step: 2 \ttraining Acc: 0.875\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.366067833633029\n",
            "Inner Loss:  0.013755194510200195\n",
            "Inner Loss:  0.0012693059148399957\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.4543766624161175\n",
            "Inner Loss:  0.009940002379672868\n",
            "Inner Loss:  0.0026179130987397264\n",
            "Step: 3 \ttraining Acc: 0.8999999999999999\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.6547987283140954\n",
            "Inner Loss:  0.07211214676499367\n",
            "Inner Loss:  0.008716958308858531\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.21388524305075407\n",
            "Inner Loss:  0.003402103004711015\n",
            "Inner Loss:  0.0007328668309907828\n",
            "Step: 4 \ttraining Acc: 0.875\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.26156572086204377\n",
            "Inner Loss:  0.0028070906243686166\n",
            "Inner Loss:  0.0005774659054752972\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.21302513063086995\n",
            "Inner Loss:  0.007342623413673469\n",
            "Inner Loss:  0.0007181259445912604\n",
            "Step: 0 \ttraining Acc: 0.8500000000000001\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.06350432536731075\n",
            "Inner Loss:  0.0004630165390803346\n",
            "Inner Loss:  0.0001509713744910966\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.389176877747689\n",
            "Inner Loss:  0.004156593200085419\n",
            "Inner Loss:  0.0013097032621902014\n",
            "Step: 1 \ttraining Acc: 0.875\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.3110329091016735\n",
            "Inner Loss:  0.013377147394099407\n",
            "Inner Loss:  0.001606456353329122\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.2826515723552023\n",
            "Inner Loss:  0.008359798163707768\n",
            "Inner Loss:  0.0017798186579187\n",
            "Step: 2 \ttraining Acc: 0.875\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.2949900872606252\n",
            "Inner Loss:  0.009849640274686473\n",
            "Inner Loss:  0.001830557843537203\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.25949743745981585\n",
            "Inner Loss:  0.004052629927173257\n",
            "Inner Loss:  0.0008651887349385236\n",
            "Step: 3 \ttraining Acc: 0.95\n",
            "----Task 0 ----\n",
            "Inner Loss:  0.3559188023209572\n",
            "Inner Loss:  0.007058357886437859\n",
            "Inner Loss:  0.001738506036677531\n",
            "----Task 1 ----\n",
            "Inner Loss:  0.4100984924339822\n",
            "Inner Loss:  0.012675024036850249\n",
            "Inner Loss:  0.0026797482278198004\n",
            "Step: 4 \ttraining Acc: 0.95\n"
          ]
        }
      ],
      "source": [
        "global_step = 0\n",
        "\n",
        "for epoch in range(args.meta_epoch):\n",
        "    \n",
        "    train = MetaTask(train_examples, num_task = 10, k_support=80, k_query=20, tokenizer = tokenizer)\n",
        "    db = create_batch_of_tasks(train, is_shuffle = True, batch_size = args.outer_batch_size)\n",
        "\n",
        "    for step, task_batch in enumerate(db):\n",
        "        \n",
        "        f = open('log.txt', 'a')\n",
        "        \n",
        "        acc = learner(task_batch)\n",
        "        \n",
        "        print('Step:', step, '\\ttraining Acc:', acc)\n",
        "        f.write(str(acc) + '\\n')\n",
        "        \n",
        "        if global_step % 20 == 0:\n",
        "            random_seed(123)\n",
        "            print(\"\\n-----------------Testing Mode-----------------\\n\")\n",
        "            db_test = create_batch_of_tasks(test, is_shuffle = False, batch_size = 1)\n",
        "            acc_all_test = []\n",
        "\n",
        "            for test_batch in db_test:\n",
        "                acc = learner(test_batch, training = False)\n",
        "                acc_all_test.append(acc)\n",
        "\n",
        "            print('Step:', step, 'Test F1:', np.mean(acc_all_test))\n",
        "            f.write('Test' + str(np.mean(acc_all_test)) + '\\n')\n",
        "            \n",
        "            random_seed(int(time.time() % 10))\n",
        "        \n",
        "        global_step += 1\n",
        "        f.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBG1tRgamvrO"
      },
      "source": [
        "## Meta_Learner_MAML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "Kr3QPIDWm0_n"
      },
      "outputs": [],
      "source": [
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
        "from torch.optim import Adam\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from transformers import BertForSequenceClassification\n",
        "from copy import deepcopy\n",
        "import gc\n",
        "import torch\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "class Learner(nn.Module):\n",
        "    \"\"\"\n",
        "    Meta Learner\n",
        "    \"\"\"\n",
        "    def __init__(self, args):\n",
        "        \"\"\"\n",
        "        :param args:\n",
        "        \"\"\"\n",
        "        super(Learner, self).__init__()\n",
        "        \n",
        "        self.num_labels = args.num_labels\n",
        "        self.outer_batch_size = args.outer_batch_size\n",
        "        self.inner_batch_size = args.inner_batch_size\n",
        "        self.outer_update_lr  = args.outer_update_lr\n",
        "        self.inner_update_lr  = args.inner_update_lr\n",
        "        self.inner_update_step = args.inner_update_step\n",
        "        self.inner_update_step_eval = args.inner_update_step_eval\n",
        "        self.bert_model = args.bert_model\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        \n",
        "        self.model = BertForSequenceClassification.from_pretrained(self.bert_model, num_labels = self.num_labels)\n",
        "        self.outer_optimizer = Adam(self.model.parameters(), lr=self.outer_update_lr)\n",
        "        self.model.train()\n",
        "\n",
        "    def forward(self, batch_tasks, training = True):\n",
        "        \"\"\"\n",
        "        batch = [(support TensorDataset, query TensorDataset),\n",
        "                 (support TensorDataset, query TensorDataset),\n",
        "                 (support TensorDataset, query TensorDataset),\n",
        "                 (support TensorDataset, query TensorDataset)]\n",
        "        \n",
        "        # support = TensorDataset(all_input_ids, all_attention_mask, all_segment_ids, all_label_ids)\n",
        "        \"\"\"\n",
        "        task_accs = []\n",
        "        sum_gradients = []\n",
        "        num_task = len(batch_tasks)\n",
        "        num_inner_update_step = self.inner_update_step if training else self.inner_update_step_eval\n",
        "\n",
        "        for task_id, task in enumerate(batch_tasks):\n",
        "            support = task[0]\n",
        "            query   = task[1]\n",
        "            \n",
        "            fast_model = deepcopy(self.model)\n",
        "            fast_model.to(self.device)\n",
        "            support_dataloader = DataLoader(support, sampler=RandomSampler(support),\n",
        "                                            batch_size=self.inner_batch_size)\n",
        "            \n",
        "            inner_optimizer = Adam(fast_model.parameters(), lr=self.inner_update_lr)\n",
        "            fast_model.train()\n",
        "            \n",
        "            print('----Task',task_id, '----')\n",
        "            for i in range(0,num_inner_update_step):\n",
        "                all_loss = []\n",
        "                for inner_step, batch in enumerate(support_dataloader):\n",
        "                    \n",
        "                    batch = tuple(t.to(self.device) for t in batch)\n",
        "                    input_ids, attention_mask, segment_ids, label_id = batch\n",
        "                    outputs = fast_model(input_ids, attention_mask, segment_ids, labels = label_id)\n",
        "                    \n",
        "                    loss = outputs[0]              \n",
        "                    loss.backward()\n",
        "                    inner_optimizer.step()\n",
        "                    inner_optimizer.zero_grad()\n",
        "                    \n",
        "                    all_loss.append(loss.item())\n",
        "                \n",
        "                if i % 4 == 0:\n",
        "                    print(\"Inner Loss: \", np.mean(all_loss))\n",
        "\n",
        "            query_dataloader = DataLoader(query, sampler=None, batch_size=len(query))\n",
        "            query_batch = iter(query_dataloader).next()\n",
        "            query_batch = tuple(t.to(self.device) for t in query_batch)\n",
        "            q_input_ids, q_attention_mask, q_segment_ids, q_label_id = query_batch\n",
        "            q_outputs = fast_model(q_input_ids, q_attention_mask, q_segment_ids, labels = q_label_id)\n",
        "            \n",
        "            if training:\n",
        "                q_loss = q_outputs[0]\n",
        "                q_loss.backward()\n",
        "                fast_model.to(torch.device('cpu'))\n",
        "                for i, params in enumerate(fast_model.parameters()):\n",
        "                    if task_id == 0:\n",
        "                        sum_gradients.append(deepcopy(params.grad))\n",
        "                    else:\n",
        "                        sum_gradients[i] += deepcopy(params.grad)\n",
        "\n",
        "            q_logits = F.softmax(q_outputs[1],dim=1)\n",
        "            pre_label_id = torch.argmax(q_logits,dim=1)\n",
        "            pre_label_id = pre_label_id.detach().cpu().numpy().tolist()\n",
        "            q_label_id = q_label_id.detach().cpu().numpy().tolist()\n",
        "            \n",
        "            acc = accuracy_score(pre_label_id,q_label_id)\n",
        "            task_accs.append(acc)\n",
        "            \n",
        "            del fast_model, inner_optimizer\n",
        "            torch.cuda.empty_cache()\n",
        "        \n",
        "        if training:\n",
        "            # Average gradient across tasks\n",
        "            for i in range(0,len(sum_gradients)):\n",
        "                sum_gradients[i] = sum_gradients[i] / float(num_task)\n",
        "\n",
        "            #Assign gradient for original model, then using optimizer to update its weights\n",
        "            for i, params in enumerate(self.model.parameters()):\n",
        "                params.grad = sum_gradients[i]\n",
        "\n",
        "            self.outer_optimizer.step()\n",
        "            self.outer_optimizer.zero_grad()\n",
        "            \n",
        "            del sum_gradients\n",
        "            gc.collect()\n",
        "        \n",
        "        return np.mean(task_accs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUD3kTATj4pL"
      },
      "source": [
        "## Functional_Forward_Bert"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "I-XVvFzgiGaW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f71502b2-dd92-463b-eaa5-bb9a22bb6beb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([[[-0.1576,  0.3370, -0.1221,  ..., -0.3356,  0.3942,  0.4703],\n",
            "         [ 0.8703,  0.3434,  0.3536,  ..., -0.3917,  0.7031,  1.0644],\n",
            "         [ 0.1744,  1.0315,  0.2739,  ..., -0.1700,  0.6216, -0.0226],\n",
            "         ...,\n",
            "         [ 0.3732,  0.2495,  0.4426,  ..., -0.1940,  0.3933,  0.6892],\n",
            "         [-0.0438,  0.3092,  0.0463,  ...,  0.1204,  0.3048, -0.0807],\n",
            "         [ 0.5353,  0.2694, -0.4089,  ...,  0.5772, -0.6564, -0.3425]],\n",
            "\n",
            "        [[-0.4248, -0.3055,  0.3123,  ...,  0.0534,  0.3531,  0.3712],\n",
            "         [ 0.1217,  0.0371,  0.1795,  ..., -0.1514,  0.6819,  0.1585],\n",
            "         [ 0.9209,  0.1281,  0.2641,  ..., -0.2881,  0.5027,  0.3020],\n",
            "         ...,\n",
            "         [ 0.1037, -0.6846,  0.0244,  ..., -0.3091,  0.2962,  0.5372],\n",
            "         [-0.3636, -0.3467, -0.0437,  ...,  0.1067,  0.0879, -0.0273],\n",
            "         [ 0.4633,  0.0210, -0.3303,  ...,  0.2166, -0.4741, -0.0168]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>),)\n"
          ]
        }
      ],
      "source": [
        "from torch.nn.functional import gelu, elu\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import math\n",
        "import torch\n",
        "from collections import OrderedDict\n",
        "from transformers import BertModel, BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "def functional_bert(fast_weights, config, input_ids=None, attention_mask=None, token_type_ids=None,\n",
        "                    position_ids=None, head_mask=None, inputs_embeds=None, encoder_hidden_states=None,\n",
        "                    encoder_attention_mask=None, is_train = True):\n",
        "\n",
        "    if input_ids is not None and inputs_embeds is not None:\n",
        "        raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
        "    elif input_ids is not None:\n",
        "        input_shape = input_ids.size()\n",
        "    elif inputs_embeds is not None:\n",
        "        input_shape = inputs_embeds.size()[:-1]\n",
        "    else:\n",
        "        raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "    device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
        "\n",
        "    if attention_mask is None:\n",
        "        attention_mask = torch.ones(input_shape, device=device)\n",
        "    if token_type_ids is None:\n",
        "        token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
        "\n",
        "    if attention_mask.dim() == 3:\n",
        "        extended_attention_mask = attention_mask[:, None, :, :]\n",
        "    elif attention_mask.dim() == 2:\n",
        "        if config.is_decoder:\n",
        "            batch_size, seq_length = input_shape\n",
        "            seq_ids = torch.arange(seq_length, device=device)\n",
        "            causal_mask = seq_ids[None, None, :].repeat(batch_size, seq_length, 1) <= seq_ids[None, :, None]\n",
        "            causal_mask = causal_mask.to(torch.long)\n",
        "            extended_attention_mask = causal_mask[:, None, :, :] * attention_mask[:, None, None, :]\n",
        "        else:\n",
        "            extended_attention_mask = attention_mask[:, None, None, :]\n",
        "    else:\n",
        "        raise ValueError(\"Wrong shape for input_ids (shape {}) or attention_mask (shape {})\".format(input_shape, attention_mask.shape))\n",
        "\n",
        "    extended_attention_mask = extended_attention_mask.to(dtype=next((p for p in fast_weights.values())).dtype)  # fp16 compatibility\n",
        "    extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "    if config.is_decoder and encoder_hidden_states is not None:\n",
        "        encoder_batch_size, encoder_sequence_length, _ = encoder_hidden_states.size()\n",
        "        encoder_hidden_shape = (encoder_batch_size, encoder_sequence_length)\n",
        "        if encoder_attention_mask is None:\n",
        "            encoder_attention_mask = torch.ones(encoder_hidden_shape, device=device)\n",
        "\n",
        "        if encoder_attention_mask.dim() == 3:\n",
        "            encoder_extended_attention_mask = encoder_attention_mask[:, None, :, :]\n",
        "        elif encoder_attention_mask.dim() == 2:\n",
        "            encoder_extended_attention_mask = encoder_attention_mask[:, None, None, :]\n",
        "        else:\n",
        "            raise ValueError(\"Wrong shape for encoder_hidden_shape (shape {}) or encoder_attention_mask (shape {})\".format(encoder_hidden_shape,\n",
        "                                                                                                                           encoder_attention_mask.shape))\n",
        "        encoder_extended_attention_mask = encoder_extended_attention_mask.to(dtype=next((p for p in fast_weights.values())).dtype) \n",
        "        encoder_extended_attention_mask = (1.0 - encoder_extended_attention_mask) * -10000.0\n",
        "    else:\n",
        "        encoder_extended_attention_mask = None\n",
        "\n",
        "    if head_mask is not None:\n",
        "        if head_mask.dim() == 1:\n",
        "            head_mask = head_mask.unsqueeze(0).unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n",
        "            head_mask = head_mask.expand(config.num_hidden_layers, -1, -1, -1, -1)\n",
        "        elif head_mask.dim() == 2:\n",
        "            head_mask = head_mask.unsqueeze(1).unsqueeze(-1).unsqueeze(-1)\n",
        "        head_mask = head_mask.to(dtype=next((p for p in fast_weights.values())).dtype)\n",
        "    else:\n",
        "        head_mask = [None] * config.num_hidden_layers\n",
        "    \n",
        "    embedding_output = functional_embeeding(fast_weights, config, input_ids, position_ids, \n",
        "                                            token_type_ids, inputs_embeds, is_train = is_train)\n",
        "    \n",
        "    encoder_outputs = functional_encoder(fast_weights, config, embedding_output,\n",
        "                                   attention_mask=extended_attention_mask,\n",
        "                                   head_mask=head_mask, encoder_hidden_states=encoder_hidden_states,\n",
        "                                   encoder_attention_mask=encoder_extended_attention_mask, is_train = is_train)\n",
        "    \n",
        "    sequence_output = encoder_outputs\n",
        "    outputs = (sequence_output,)\n",
        "    return outputs\n",
        "\n",
        "\n",
        "def functional_embeeding(fast_weights, config, input_ids, position_ids, \n",
        "                         token_type_ids, inputs_embeds = None, is_train = True):\n",
        "\n",
        "    if input_ids is not None:\n",
        "        input_shape = input_ids.size()\n",
        "    else:\n",
        "        input_shape = inputs_embeds.size()[:-1]\n",
        "\n",
        "    seq_length = input_shape[1]\n",
        "    device = input_ids.device if input_ids is not None else inputs_embeds.device\n",
        "    if position_ids is None:\n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long, device=device)\n",
        "        position_ids = position_ids.unsqueeze(0).expand(input_shape)\n",
        "    if token_type_ids is None:\n",
        "        token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n",
        "\n",
        "    if inputs_embeds is None:\n",
        "        inputs_embeds = F.embedding(input_ids, fast_weights['bert.embeddings.word_embeddings.weight'], padding_idx = 0)\n",
        "    \n",
        "    position_embeddings = F.embedding(position_ids, fast_weights['bert.embeddings.position_embeddings.weight'])\n",
        "    token_type_embeddings = F.embedding(token_type_ids, fast_weights['bert.embeddings.token_type_embeddings.weight'])\n",
        "\n",
        "    embeddings = inputs_embeds + position_embeddings + token_type_embeddings\n",
        "    \n",
        "    embeddings = F.layer_norm(embeddings, [config.hidden_size], \n",
        "                              weight=fast_weights['bert.embeddings.LayerNorm.weight'],\n",
        "                              bias=fast_weights['bert.embeddings.LayerNorm.bias'],\n",
        "                              eps=config.layer_norm_eps)\n",
        "\n",
        "    embeddings = F.dropout(embeddings, p=config.hidden_dropout_prob, training = is_train)\n",
        "    \n",
        "    return embeddings\n",
        "\n",
        "    \n",
        "def transpose_for_scores(config, x):\n",
        "    new_x_shape = x.size()[:-1] + (config.num_attention_heads, int(config.hidden_size / config.num_attention_heads))\n",
        "    x = x.view(*new_x_shape)\n",
        "    return x.permute(0, 2, 1, 3)\n",
        "\n",
        "def functional_self_attention(fast_weights, config, layer_idx,\n",
        "                              hidden_states, attention_mask, head_mask, \n",
        "                              encoder_hidden_states, encoder_attention_mask,\n",
        "                              is_train = True):\n",
        "    \n",
        "    attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "    all_head_size = config.num_attention_heads * attention_head_size\n",
        "    \n",
        "    mixed_query_layer = F.linear(hidden_states,\n",
        "                                fast_weights['bert.encoder.layer.'+layer_idx+'.attention.self.query.weight'],\n",
        "                                fast_weights['bert.encoder.layer.'+layer_idx+'.attention.self.query.bias'])\n",
        "    \n",
        "    if encoder_hidden_states is not None:\n",
        "        mixed_key_layer = F.linear(encoder_hidden_states,\n",
        "                                fast_weights['bert.encoder.layer.'+layer_idx+'.attention.self.key.weight'],\n",
        "                                fast_weights['bert.encoder.layer.'+layer_idx+'.attention.self.key.bias'])\n",
        "        mixed_value_layer = F.linear(encoder_hidden_states,\n",
        "                                fast_weights['bert.encoder.layer.'+layer_idx+'.attention.self.value.weight'],\n",
        "                                fast_weights['bert.encoder.layer.'+layer_idx+'.attention.self.value.bias'])\n",
        "        attention_mask = encoder_attention_mask\n",
        "    else:\n",
        "        mixed_key_layer   = F.linear(hidden_states,\n",
        "                                fast_weights['bert.encoder.layer.'+layer_idx+'.attention.self.key.weight'],\n",
        "                                fast_weights['bert.encoder.layer.'+layer_idx+'.attention.self.key.bias'])\n",
        "        mixed_value_layer = F.linear(hidden_states,\n",
        "                                fast_weights['bert.encoder.layer.'+layer_idx+'.attention.self.value.weight'],\n",
        "                                fast_weights['bert.encoder.layer.'+layer_idx+'.attention.self.value.bias'])\n",
        "\n",
        "    query_layer = transpose_for_scores(config, mixed_query_layer)\n",
        "    key_layer   = transpose_for_scores(config, mixed_key_layer)\n",
        "    value_layer = transpose_for_scores(config, mixed_value_layer)\n",
        "\n",
        "    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "    attention_scores = attention_scores / math.sqrt(attention_head_size)\n",
        "    if attention_mask is not None:\n",
        "        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
        "        attention_scores = attention_scores + attention_mask\n",
        "        \n",
        "    attention_probs = torch.nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "    if is_train:\n",
        "        attention_probs = F.dropout(attention_probs, p= config.attention_probs_dropout_prob)\n",
        "\n",
        "    # Mask heads if we want to\n",
        "    if head_mask is not None:\n",
        "        attention_probs = attention_probs * head_mask\n",
        "    \n",
        "    context_layer = torch.matmul(attention_probs, value_layer)\n",
        "\n",
        "    context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "    new_context_layer_shape = context_layer.size()[:-2] + (all_head_size,)\n",
        "    context_layer = context_layer.view(*new_context_layer_shape)\n",
        "    \n",
        "    outputs = context_layer\n",
        "    return outputs\n",
        "    \n",
        "def functional_out_attention(fast_weights, config, layer_idx,\n",
        "                              hidden_states, input_tensor,\n",
        "                              is_train = True):\n",
        "    \n",
        "    hidden_states = F.linear(hidden_states,\n",
        "                            fast_weights['bert.encoder.layer.'+layer_idx+'.attention.output.dense.weight'],\n",
        "                            fast_weights['bert.encoder.layer.'+layer_idx+'.attention.output.dense.bias'])\n",
        "\n",
        "    hidden_states = F.dropout(hidden_states, p=config.hidden_dropout_prob, training = is_train)\n",
        "    hidden_states = F.layer_norm(hidden_states + input_tensor, [config.hidden_size],\n",
        "                              weight=fast_weights['bert.encoder.layer.'+layer_idx+'.attention.output.LayerNorm.weight'],\n",
        "                              bias=fast_weights['bert.encoder.layer.'+layer_idx+'.attention.output.LayerNorm.bias'],\n",
        "                              eps=config.layer_norm_eps)\n",
        "    \n",
        "    return hidden_states    \n",
        "\n",
        "\n",
        "def functional_attention(fast_weights, config, layer_idx,\n",
        "                         hidden_states, attention_mask=None, head_mask=None,\n",
        "                         encoder_hidden_states=None, encoder_attention_mask=None,\n",
        "                         is_train = True):\n",
        "    \n",
        "    self_outputs = functional_self_attention(fast_weights, config, layer_idx,\n",
        "                                             hidden_states, attention_mask, head_mask, \n",
        "                                             encoder_hidden_states, encoder_attention_mask, is_train)\n",
        "    \n",
        "    attention_output = functional_out_attention(fast_weights, config, layer_idx,\n",
        "                                                self_outputs, hidden_states, is_train)\n",
        "    return attention_output\n",
        "\n",
        "def functional_intermediate(fast_weights, config, layer_idx, hidden_states, is_train = True):\n",
        "    weight_name = 'bert.encoder.layer.' + layer_idx + '.intermediate.dense.weight'\n",
        "    bias_name   = 'bert.encoder.layer.' + layer_idx + '.intermediate.dense.bias'\n",
        "    hidden_states = F.linear(hidden_states, fast_weights[weight_name], fast_weights[bias_name])\n",
        "    hidden_states = gelu(hidden_states)\n",
        "    \n",
        "    return hidden_states\n",
        "\n",
        "\n",
        "def functional_output(fast_weights, config, layer_idx, hidden_states, input_tensor, is_train = True):\n",
        "\n",
        "    hidden_states = F.linear(hidden_states, \n",
        "                             fast_weights['bert.encoder.layer.'+layer_idx+'.output.dense.weight'], \n",
        "                             fast_weights['bert.encoder.layer.'+layer_idx+'.output.dense.bias'])\n",
        "    \n",
        "    hidden_states = F.dropout(hidden_states, p=config.hidden_dropout_prob, training = is_train)\n",
        "    hidden_states = F.layer_norm(hidden_states + input_tensor, [config.hidden_size],\n",
        "                              weight=fast_weights['bert.encoder.layer.'+layer_idx+'.output.LayerNorm.weight'],\n",
        "                              bias=fast_weights['bert.encoder.layer.'+layer_idx+'.output.LayerNorm.bias'],\n",
        "                              eps=config.layer_norm_eps)\n",
        "    return hidden_states\n",
        "\n",
        "def functional_layer(fast_weights, config, layer_idx, hidden_states, attention_mask,\n",
        "                     head_mask, encoder_hidden_states, encoder_attention_mask, is_train = True):\n",
        "    \n",
        "    self_attention_outputs = functional_attention(fast_weights, config, layer_idx,\n",
        "                                                  hidden_states, attention_mask, head_mask,\n",
        "                                                  encoder_hidden_states, encoder_attention_mask,is_train)\n",
        "    \n",
        "    attention_output = self_attention_outputs\n",
        "    intermediate_output = functional_intermediate(fast_weights, config, layer_idx, attention_output, is_train)\n",
        "    layer_output = functional_output(fast_weights, config, layer_idx, \n",
        "                                     intermediate_output, attention_output, is_train)\n",
        "    \n",
        "    return layer_output\n",
        "    \n",
        "\n",
        "def functional_encoder(fast_weights, config , hidden_states, attention_mask,\n",
        "                       head_mask, encoder_hidden_states, encoder_attention_mask, is_train = True):\n",
        "    \n",
        "    for i in range(0,config.num_hidden_layers):\n",
        "        layer_outputs = functional_layer(fast_weights, config, str(i),\n",
        "                                         hidden_states, attention_mask, head_mask[i], \n",
        "                                         encoder_hidden_states, encoder_attention_mask, is_train)\n",
        "        hidden_states = layer_outputs\n",
        "        \n",
        "    outputs = hidden_states\n",
        "    return outputs\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "    model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "    fast_weights = OrderedDict(model.named_parameters())\n",
        "    \n",
        "    input_ids = torch.Tensor([[  101,  1303,  1110,  1199,  3087,  1106,  4035, 13775,   102],\n",
        "                              [  101,   178,  1274,  1204,  1176,  1115,  4170,   182,   102]]).to(torch.long)\n",
        "    token_type_ids = torch.Tensor([[0,  0,  0,  0,  0,  1,  1, 1, 1],\n",
        "                                   [0,  0,  0,  0,  0,  1,  1, 1, 1]]).to(torch.long)\n",
        "    attention_mask = torch.Tensor([[1,  1,  1,  1,  1,  1,  1, 1, 1],\n",
        "                                   [1,  1,  1,  1,  1,  1,  1, 1, 1]]).to(torch.long)\n",
        "    \n",
        "    print(functional_bert(fast_weights, model.config, input_ids=input_ids, attention_mask=attention_mask, \n",
        "                    token_type_ids=token_type_ids,is_train = True))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}